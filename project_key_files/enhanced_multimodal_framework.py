#!/usr/bin/env python3
"""
å¢žå¼ºç‰ˆå¤šæ¨¡æ€èžåˆæ¡†æž¶
- åˆ†å±‚æ ‡è®°åŒ–è¿‡ç¨‹ï¼ˆæ— éœ€VAEï¼‰
- å¤šæ¨¡æ€æŽ©ç æœºåˆ¶
- å¤šç²’åº¦ä¸€è‡´æ€§è·¨æ¨¡æ€èžåˆ
- å†…å­˜ä¼˜åŒ–ï¼Œæ”¯æŒ4å¡GPU
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import numpy as np
from typing import Optional, Tuple, List, Dict, Union

# å°è¯•å¯¼å…¥timmç”¨äºŽConvNeXt
try:
    import timm
    TIMM_AVAILABLE = True
except ImportError:
    TIMM_AVAILABLE = False
    print("è­¦å‘Š: timmåº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€åŒ–ResNetã€‚å»ºè®®å®‰è£…: pip install timm")

try:
    from torchvision.models import resnet34, ResNet34_Weights
except ImportError:
    # å¦‚æžœæ²¡æœ‰torchvisionï¼Œåˆ›å»ºä¸€ä¸ªç®€åŒ–çš„ResNet
    pass

class HierarchicalTokenizer(nn.Module):
    """åˆ†å±‚æ ‡è®°åŒ–è¿‡ç¨‹ - ç›´æŽ¥ä»Žåƒç´ çº§åˆ°å—çº§åµŒå…¥"""
    
    def __init__(self, in_channels: int, embed_dim: int, patch_size: int = 4):
        super().__init__()
        self.patch_size = patch_size
        self.embed_dim = embed_dim
        
        # åƒç´ çº§åˆ°å—çº§åµŒå…¥
        self.pixel_to_patch = nn.Conv2d(
            in_channels, embed_dim, 
            kernel_size=patch_size, 
            stride=patch_size
        )
        
        # ä½ç½®ç¼–ç  - åŠ¨æ€å¤§å°
        self.pos_embed = None  # å°†åœ¨forwardä¸­åŠ¨æ€åˆ›å»º
        
        # å±‚å½’ä¸€åŒ–
        self.norm = nn.LayerNorm(embed_dim)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, C, H, W = x.shape
        
        # åƒç´ çº§åˆ°å—çº§åµŒå…¥
        x = self.pixel_to_patch(x)  # (B, embed_dim, H//patch_size, W//patch_size)
        
        # åŠ¨æ€åˆ›å»ºä½ç½®ç¼–ç 
        h, w = x.shape[2], x.shape[3]
        if self.pos_embed is None or self.pos_embed.shape[-2:] != (h, w):
            self.pos_embed = nn.Parameter(torch.zeros(1, self.embed_dim, h, w, device=x.device))
            nn.init.trunc_normal_(self.pos_embed, std=0.02)
        x = x + self.pos_embed
        
        # è½¬æ¢ä¸ºåºåˆ—æ ¼å¼
        x = x.flatten(2).transpose(1, 2)  # (B, N, embed_dim)
        
        return self.norm(x.contiguous())

class MultiModalMasking(nn.Module):
    """å¤šæ¨¡æ€æŽ©ç æœºåˆ¶"""
    
    def __init__(self, embed_dim: int, mask_ratio: float = 0.15):
        super().__init__()
        self.embed_dim = embed_dim
        self.mask_ratio = mask_ratio
        
        # æŽ©ç token
        self.mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        
        # æ¨¡æ€ç‰¹å®šçš„æŽ©ç é¢„æµ‹å™¨
        self.mask_predictor = nn.Sequential(
            nn.Linear(embed_dim, embed_dim // 4),
            nn.GELU(),
            nn.Linear(embed_dim // 4, 1),
            nn.Sigmoid()
        )
        
    def forward(self, x: torch.Tensor, modality: str = 'rgb') -> Tuple[torch.Tensor, torch.Tensor]:
        B, N, D = x.shape
        
        # ç”ŸæˆæŽ©ç 
        mask_prob = self.mask_predictor(x)  # (B, N, 1)
        mask = torch.bernoulli(mask_prob).bool()  # (B, N, 1)
        
        # åº”ç”¨æŽ©ç 
        masked_x = x.clone()
        mask_squeezed = mask.squeeze(-1)
        masked_x[mask_squeezed] = self.mask_token
        
        return masked_x.contiguous(), mask_squeezed

class MultiGranularityConsistencyFusion(nn.Module):
    """å¤šç²’åº¦ä¸€è‡´æ€§èžåˆæ¨¡å— - é¦–æ¬¡æå‡º
    æž„å»ºå¤šå°ºåº¦ç²’åº¦çš„ç‰¹å¾è¡¨ç¤ºï¼Œé€šè¿‡ç²’åº¦é€‰æ‹©æœºåˆ¶åŠ¨æ€é€‰æ‹©æœ€åŒ¹é…çš„è¯­ä¹‰å±‚æ¬¡
    åˆ›æ–°ç‚¹ï¼š
    1. å¤šç²’åº¦ç‰¹å¾è¡¨ç¤ºï¼ˆåƒç´ çº§ã€å¯¹è±¡çº§ã€åŒºåŸŸçº§ï¼‰
    2. åŠ¨æ€ç²’åº¦é€‰æ‹©æœºåˆ¶ï¼ˆæ ¹æ®ç‰¹å¾å†…å®¹è‡ªé€‚åº”é€‰æ‹©ï¼‰
    3. ç²’åº¦ä¸€è‡´æ€§çº¦æŸï¼ˆç¡®ä¿ä¸åŒç²’åº¦é—´çš„ä¸€è‡´æ€§ï¼‰
    """
    
    def __init__(self, embed_dim: int, num_granularities: int = 4):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_granularities = num_granularities
        
        # å¤šç²’åº¦ç‰¹å¾æž„å»ºå™¨ï¼ˆä¸åŒè¯­ä¹‰å±‚æ¬¡ï¼‰
        # granularity 0: åƒç´ çº§ (scale=1)
        # granularity 1: å¯¹è±¡çº§ (scale=2)  
        # granularity 2: åŒºåŸŸçº§ (scale=4)
        # granularity 3: åœºæ™¯çº§ (scale=8)
        self.granularity_extractors = nn.ModuleList([
            self._create_granularity_extractor(embed_dim, scale) 
            for scale in [1, 2, 4, 8]
        ])
        
        # åŠ¨æ€ç²’åº¦é€‰æ‹©æœºåˆ¶ï¼ˆåŸºäºŽç‰¹å¾å†…å®¹è‡ªé€‚åº”é€‰æ‹©ï¼‰
        self.granularity_selector = nn.Sequential(
            nn.Linear(embed_dim * 2, embed_dim),
            nn.GELU(),
            nn.Linear(embed_dim, embed_dim // 2),
            nn.GELU(),
            nn.Linear(embed_dim // 2, num_granularities),
            nn.Softmax(dim=-1)
        )
        
        # ç²’åº¦ä¸€è‡´æ€§çº¦æŸç½‘ç»œï¼ˆç¡®ä¿ä¸åŒç²’åº¦ç‰¹å¾çš„ä¸€è‡´æ€§ï¼‰
        self.consistency_net = nn.Sequential(
            nn.Conv2d(embed_dim * num_granularities, embed_dim * 2, kernel_size=3, padding=1),
            nn.BatchNorm2d(embed_dim * 2),
            nn.GELU(),
            nn.Conv2d(embed_dim * 2, embed_dim, kernel_size=1),
            nn.BatchNorm2d(embed_dim),
            nn.GELU()
        )
        
        # è‡ªé€‚åº”èžåˆæƒé‡ï¼ˆæ ¹æ®ç²’åº¦é€‰æ‹©æƒé‡åŠ¨æ€èžåˆï¼‰
        self.adaptive_weights = nn.Sequential(
            nn.Linear(embed_dim * 2, embed_dim),
            nn.GELU(),
            nn.Linear(embed_dim, num_granularities),
            nn.Sigmoid()
        )
        
        # è¾“å‡ºæŠ•å½±
        self.output_proj = nn.Sequential(
            nn.Linear(embed_dim, embed_dim),
            nn.LayerNorm(embed_dim),
            nn.GELU()
        )
        
    def _create_granularity_extractor(self, embed_dim: int, scale: int) -> nn.Module:
        """åˆ›å»ºä¸åŒç²’åº¦çš„ç‰¹å¾æå–å™¨"""
        layers = [
            nn.Conv2d(embed_dim * 2, embed_dim, kernel_size=3, padding=1),
            nn.BatchNorm2d(embed_dim),
            nn.GELU()
        ]
        
        if scale > 1:
            if scale == 8:
                # åœºæ™¯çº§ï¼šä½¿ç”¨å…¨å±€æ± åŒ–
                layers.extend([
                    nn.AdaptiveAvgPool2d(1)
                ])
            else:
                # å¯¹è±¡çº§ã€åŒºåŸŸçº§ï¼šä½¿ç”¨ä¸‹é‡‡æ ·
                layers.extend([
                    nn.Conv2d(embed_dim, embed_dim, kernel_size=scale, stride=scale),
                    nn.BatchNorm2d(embed_dim),
                    nn.GELU()
                ])
        
        return nn.Sequential(*layers)
    
    def forward(self, rgb_features: torch.Tensor, dsm_features: torch.Tensor) -> torch.Tensor:
        B, N, D = rgb_features.shape
        H = W = int(math.sqrt(N))
        
        # é‡å¡‘ä¸ºç©ºé—´æ ¼å¼
        rgb_spatial = rgb_features.transpose(1, 2).view(B, D, H, W)
        dsm_spatial = dsm_features.transpose(1, 2).view(B, D, H, W)
        concat_spatial = torch.cat([rgb_spatial, dsm_spatial], dim=1)  # (B, 2D, H, W)
        
        # æž„å»ºå¤šç²’åº¦ç‰¹å¾è¡¨ç¤º
        granularity_features = []
        for extractor in self.granularity_extractors:
            feat = extractor(concat_spatial)  # (B, D, H_gran, W_gran)
            
            # ç¡®ä¿æ‰€æœ‰ç‰¹å¾éƒ½ä¸Šé‡‡æ ·åˆ°ç›¸åŒå°ºå¯¸ä»¥ä¾¿èžåˆ
            if feat.shape[2:] != (H, W):
                feat = F.interpolate(feat, size=(H, W), mode='bilinear', align_corners=False)
            granularity_features.append(feat)
        
        # è®¡ç®—åŠ¨æ€ç²’åº¦é€‰æ‹©æƒé‡ï¼ˆåŸºäºŽç‰¹å¾å†…å®¹ï¼‰
        concat_seq = torch.cat([rgb_features, dsm_features], dim=-1).contiguous()  # (B, N, 2D)
        granularity_weights = self.granularity_selector(concat_seq)  # (B, N, num_granularities)
        
        # è®¡ç®—è‡ªé€‚åº”èžåˆæƒé‡
        avg_features = torch.mean(concat_seq, dim=1).contiguous()  # (B, 2D)
        adaptive_weights = self.adaptive_weights(avg_features)  # (B, num_granularities)
        
        # åŠ æƒèžåˆå¤šç²’åº¦ç‰¹å¾
        fused_multi_gran = torch.zeros_like(granularity_features[0]).contiguous()
        for i, gran_feat in enumerate(granularity_features):
            # èŽ·å–é€‰æ‹©å™¨æƒé‡å’Œè‡ªé€‚åº”æƒé‡
            selector_w = granularity_weights[:, :, i].contiguous()  # (B, N)
            adaptive_w = adaptive_weights[:, i].contiguous()  # (B,)
            
            # è½¬æ¢ä¸ºç©ºé—´æ ¼å¼å¹¶åŠ æƒ
            selector_w_spatial = selector_w.view(B, 1, H, W).contiguous()
            adaptive_w_spatial = adaptive_w.view(B, 1, 1, 1).expand(-1, 1, H, W).contiguous()
            weight = selector_w_spatial * adaptive_w_spatial
            
            # ç¡®ä¿gran_featå°ºå¯¸åŒ¹é…
            if gran_feat.shape[2:] != (H, W):
                gran_feat = F.interpolate(gran_feat, size=(H, W), mode='bilinear', align_corners=False).contiguous()
            
            fused_multi_gran = fused_multi_gran + weight * gran_feat
        
        # ç²’åº¦ä¸€è‡´æ€§çº¦æŸï¼ˆç¡®ä¿ä¸åŒç²’åº¦ç‰¹å¾çš„ä¸€è‡´æ€§ï¼‰
        all_gran_features = torch.cat(granularity_features, dim=1).contiguous()  # (B, D*num_gran, H, W)
        consistency_features = self.consistency_net(all_gran_features).contiguous()  # (B, D, H, W)
        
        # èžåˆä¸€è‡´æ€§ç‰¹å¾å’ŒåŠ æƒç‰¹å¾
        final_spatial = fused_multi_gran + 0.3 * consistency_features
        
        # è½¬æ¢å›žåºåˆ—æ ¼å¼
        final_features = final_spatial.view(B, D, N).transpose(1, 2).contiguous()
        
        return self.output_proj(final_features).contiguous()

class ArbitraryModalityAdapter(nn.Module):
    """ä»»æ„æ¨¡æ€é€‚é…å™¨ - é¦–æ¬¡æå‡º
    æ”¯æŒ1-5ä¸ªæ¨¡æ€çµæ´»è¾“å…¥ï¼ŒåŠ¨æ€å¤„ç†ä¸åŒæ•°é‡å’Œç±»åž‹çš„æ¨¡æ€æ•°æ®
    åˆ›æ–°ç‚¹ï¼š
    1. æ¨¡æ€æ— å…³çš„è¾“å…¥æŽ¥å£ï¼ˆæ”¯æŒRGBã€DSMã€SARã€HSIã€LiDARç­‰ï¼‰
    2. åŠ¨æ€æ¨¡æ€èžåˆï¼ˆæ ¹æ®å¯ç”¨æ¨¡æ€æ•°é‡è‡ªé€‚åº”è°ƒæ•´ï¼‰
    3. æ¨¡æ€ç¼ºå¤±è¡¥å¿æœºåˆ¶ï¼ˆå¤„ç†éƒ¨åˆ†æ¨¡æ€ç¼ºå¤±æƒ…å†µï¼‰
    """
    
    def __init__(self, embed_dim: int, max_modalities: int = 5):
        super().__init__()
        self.embed_dim = embed_dim
        self.max_modalities = max_modalities
        
        # æ¨¡æ€ç¼–ç å™¨ï¼ˆå°†ä¸åŒæ¨¡æ€ç‰¹å¾æ˜ å°„åˆ°ç»Ÿä¸€ç©ºé—´ï¼‰
        self.modality_encoder = nn.ModuleDict({
            'rgb': nn.Linear(embed_dim, embed_dim),
            'dsm': nn.Linear(embed_dim, embed_dim),
            'sar': nn.Linear(embed_dim, embed_dim),
            'hsi': nn.Linear(embed_dim, embed_dim),
            'lidar': nn.Linear(embed_dim, embed_dim)
        })
        
        # æ¨¡æ€å­˜åœ¨æ€§æ£€æµ‹ï¼ˆåˆ¤æ–­å“ªäº›æ¨¡æ€å¯ç”¨ï¼‰
        self.modality_presence = nn.Sequential(
            nn.Linear(embed_dim, embed_dim // 2),
            nn.GELU(),
            nn.Linear(embed_dim // 2, 1),
            nn.Sigmoid()
        )
        
        # è‡ªé€‚åº”æ¨¡æ€èžåˆç½‘ç»œï¼ˆæ ¹æ®å¯ç”¨æ¨¡æ€æ•°é‡åŠ¨æ€è°ƒæ•´ï¼‰
        self.adaptive_fusion = nn.Sequential(
            nn.Linear(embed_dim * max_modalities, embed_dim * 2),
            nn.GELU(),
            nn.Linear(embed_dim * 2, embed_dim),
            nn.LayerNorm(embed_dim),
            nn.GELU()
        )
        
        # æ¨¡æ€ç¼ºå¤±è¡¥å¿ï¼ˆå½“æŸäº›æ¨¡æ€ç¼ºå¤±æ—¶çš„è¡¥å¿ç­–ç•¥ï¼‰
        self.missing_modality_compensation = nn.Sequential(
            nn.Linear(embed_dim, embed_dim),
            nn.GELU(),
            nn.Linear(embed_dim, embed_dim)
        )
        
    def forward(self, modality_features: Dict[str, torch.Tensor]) -> torch.Tensor:
        """
        Args:
            modality_features: å­—å…¸ï¼Œé”®ä¸ºæ¨¡æ€åç§°ï¼ˆ'rgb', 'dsm', 'sar', 'hsi', 'lidar'ï¼‰ï¼Œå€¼ä¸ºç‰¹å¾tensor (B, N, D)
        Returns:
            èžåˆåŽçš„ç‰¹å¾ (B, N, D)
        """
        B, N, D = list(modality_features.values())[0].shape
        
        # ç¼–ç æ‰€æœ‰å¯ç”¨æ¨¡æ€
        encoded_features = {}
        presence_scores = {}
        
        for mod_name, mod_feat in modality_features.items():
            if mod_name in self.modality_encoder:
                # ç¼–ç åˆ°ç»Ÿä¸€ç©ºé—´
                encoded = self.modality_encoder[mod_name](mod_feat).contiguous()
                encoded_features[mod_name] = encoded
                
                # è®¡ç®—æ¨¡æ€å­˜åœ¨æ€§åˆ†æ•°
                presence = self.modality_presence(mod_feat).contiguous()  # (B, N, 1)
                presence_scores[mod_name] = presence
        
        # æž„å»ºå›ºå®šé•¿åº¦çš„ç‰¹å¾å‘é‡ï¼ˆpaddingç¼ºå¤±æ¨¡æ€ï¼‰
        all_features_list = []
        for mod_name in ['rgb', 'dsm', 'sar', 'hsi', 'lidar']:
            if mod_name in encoded_features:
                all_features_list.append(encoded_features[mod_name])
            else:
                # ç¼ºå¤±æ¨¡æ€ï¼šä½¿ç”¨é›¶å‘é‡æˆ–è¡¥å¿ç‰¹å¾
                device = encoded_features[list(encoded_features.keys())[0]].device if encoded_features else 'cpu'
                missing_feat = torch.zeros(B, N, D, device=device)
                # å¦‚æžœè‡³å°‘æœ‰ä¸€ä¸ªæ¨¡æ€å­˜åœ¨ï¼Œä½¿ç”¨è¡¥å¿ç½‘ç»œ
                if len(encoded_features) > 0:
                    avg_feat = torch.mean(torch.stack(list(encoded_features.values())), dim=0).contiguous()
                    missing_feat = self.missing_modality_compensation(avg_feat).contiguous()
                all_features_list.append(missing_feat)
        
        # æ‹¼æŽ¥æ‰€æœ‰æ¨¡æ€ç‰¹å¾
        all_features = torch.cat(all_features_list, dim=-1).contiguous()  # (B, N, D*max_modalities)
        
        # è‡ªé€‚åº”èžåˆ
        fused = self.adaptive_fusion(all_features).contiguous()  # (B, N, D)
        
        # åº”ç”¨æ¨¡æ€å­˜åœ¨æ€§æƒé‡
        if presence_scores:
            avg_presence = torch.mean(torch.stack(list(presence_scores.values())), dim=0).contiguous()
            fused = fused * avg_presence
        
        return fused.contiguous()

class SpatioTemporalAdaptiveFactor(nn.Module):
    """æ—¶ç©ºè‡ªé€‚åº”å› å­ - é¦–æ¬¡æå‡º
    åˆ©ç”¨é¥æ„Ÿæ•°æ®çš„æ—¶ç©ºç‰¹æ€§ï¼Œè‡ªé€‚åº”è°ƒæ•´ç‰¹å¾æå–å’Œèžåˆç­–ç•¥
    åˆ›æ–°ç‚¹ï¼š
    1. ç©ºé—´è‡ªé€‚åº”å› å­ï¼ˆæ ¹æ®ç©ºé—´åˆ†å¸ƒç‰¹æ€§è°ƒæ•´ï¼‰
    2. æ—¶é—´è‡ªé€‚åº”å› å­ï¼ˆå¤„ç†æ—¶åºé¥æ„Ÿæ•°æ®ï¼‰
    3. æ—¶ç©ºè€¦åˆæœºåˆ¶ï¼ˆè”åˆå»ºæ¨¡ç©ºé—´-æ—¶é—´å…³ç³»ï¼‰
    """
    
    def __init__(self, embed_dim: int):
        super().__init__()
        self.embed_dim = embed_dim
        
        # ç©ºé—´è‡ªé€‚åº”å› å­è®¡ç®—ç½‘ç»œ
        self.spatial_factor_net = nn.Sequential(
            nn.Conv2d(embed_dim, embed_dim // 2, kernel_size=3, padding=1),
            nn.BatchNorm2d(embed_dim // 2),
            nn.GELU(),
            nn.Conv2d(embed_dim // 2, 1, kernel_size=1),
            nn.Sigmoid()
        )
        
        # æ—¶é—´è‡ªé€‚åº”å› å­ï¼ˆé¢„ç•™æŽ¥å£ï¼Œå½“å‰ä¸ºå•å¸§å¤„ç†ï¼‰
        self.temporal_factor_net = nn.Sequential(
            nn.Linear(embed_dim, embed_dim // 2),
            nn.GELU(),
            nn.Linear(embed_dim // 2, 1),
            nn.Sigmoid()
        )
        
        # æ—¶ç©ºè€¦åˆç½‘ç»œï¼ˆè”åˆå»ºæ¨¡ç©ºé—´-æ—¶é—´å…³ç³»ï¼‰
        self.spatiotemporal_coupling = nn.Sequential(
            nn.Conv2d(embed_dim + 2, embed_dim, kernel_size=3, padding=1),  # +2 for spatial and temporal factors
            nn.BatchNorm2d(embed_dim),
            nn.GELU(),
            nn.Conv2d(embed_dim, embed_dim, kernel_size=1)
        )
        
        # è‡ªé€‚åº”ç‰¹å¾è°ƒåˆ¶
        self.adaptive_modulation = nn.Sequential(
            nn.Linear(embed_dim + 2, embed_dim),
            nn.GELU(),
            nn.Linear(embed_dim, embed_dim),
            nn.LayerNorm(embed_dim)
        )
        
    def forward(self, features: torch.Tensor) -> torch.Tensor:
        """
        Args:
            features: (B, N, D) æˆ– (B, D, H, W)
        Returns:
            å¢žå¼ºåŽçš„ç‰¹å¾
        """
        # è½¬æ¢åˆ°ç©ºé—´æ ¼å¼
        if len(features.shape) == 3:
            B, N, D = features.shape
            H = W = int(math.sqrt(N))
            spatial_feat = features.transpose(1, 2).contiguous().view(B, D, H, W)
            return_seq = True
        else:
            B, D, H, W = features.shape
            spatial_feat = features.contiguous()
            return_seq = False
        
        # è®¡ç®—ç©ºé—´è‡ªé€‚åº”å› å­ï¼ˆåŸºäºŽå±€éƒ¨ç©ºé—´ç‰¹å¾ï¼‰
        spatial_factor = self.spatial_factor_net(spatial_feat).contiguous()  # (B, 1, H, W)
        
        # è®¡ç®—æ—¶é—´è‡ªé€‚åº”å› å­ï¼ˆåŸºäºŽå…¨å±€ç»Ÿè®¡ç‰¹å¾ï¼Œå½“å‰ä¸ºå•å¸§ï¼‰
        global_feat = F.adaptive_avg_pool2d(spatial_feat, 1).squeeze(-1).squeeze(-1)  # (B, D)
        temporal_factor = self.temporal_factor_net(global_feat).contiguous()  # (B, 1)
        temporal_factor = temporal_factor.unsqueeze(-1).unsqueeze(-1).expand(-1, 1, H, W).contiguous()  # (B, 1, H, W)
        
        # æ—¶ç©ºè€¦åˆ
        factors = torch.cat([spatial_feat, spatial_factor, temporal_factor], dim=1).contiguous()
        coupled_feat = self.spatiotemporal_coupling(factors).contiguous()
        
        # è‡ªé€‚åº”è°ƒåˆ¶
        # è½¬æ¢ä¸ºåºåˆ—æ ¼å¼è¿›è¡Œè°ƒåˆ¶
        seq_feat = coupled_feat.view(B, D, H*W).transpose(1, 2).contiguous() if return_seq else \
                   coupled_feat.view(B, D, H*W).transpose(1, 2).contiguous()
        factors_seq = torch.cat([
            spatial_factor.view(B, H*W, 1),
            temporal_factor.view(B, H*W, 1)
        ], dim=-1).contiguous()
        
        modulated_feat = self.adaptive_modulation(
            torch.cat([seq_feat, factors_seq], dim=-1).contiguous()
        ).contiguous()
        
        return modulated_feat.contiguous()

class FeatureEnhancementNetwork(nn.Module):
    """ç‰¹å¾å¢žå¼ºç½‘ç»œ - é¦–æ¬¡æå‡º
    é€šè¿‡å¤šè·¯å¾„ç‰¹å¾å¢žå¼ºå’Œå¤šçº§ç‰¹å¾ç»†åŒ–æå‡ç‰¹å¾è¡¨è¾¾èƒ½åŠ›
    åˆ›æ–°ç‚¹ï¼š
    1. å¤šè·¯å¾„ç‰¹å¾å¢žå¼ºï¼ˆå¹¶è¡Œæå–ä¸åŒæŠ½è±¡å±‚æ¬¡çš„ç‰¹å¾ï¼‰
    2. ç‰¹å¾è‡ªæ ¡å‡†æœºåˆ¶ï¼ˆè‡ªåŠ¨æ ¡å‡†ç‰¹å¾é‡è¦æ€§ï¼‰
    3. å¤šçº§ç‰¹å¾ç»†åŒ–ï¼ˆé€æ­¥ç»†åŒ–ç‰¹å¾è¡¨ç¤ºï¼‰
    """
    
    def __init__(self, embed_dim: int):
        super().__init__()
        self.embed_dim = embed_dim
        
        # å¤šè·¯å¾„ç‰¹å¾å¢žå¼ºå™¨
        self.enhancement_paths = nn.ModuleList([
            # è·¯å¾„1: å±€éƒ¨ç‰¹å¾å¢žå¼º
            nn.Sequential(
                nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1, groups=embed_dim),
                nn.BatchNorm2d(embed_dim),
                nn.GELU(),
                nn.Conv2d(embed_dim, embed_dim, kernel_size=1),
                nn.BatchNorm2d(embed_dim)
            ),
            # è·¯å¾„2: å…¨å±€ç‰¹å¾å¢žå¼º
            nn.Sequential(
                nn.AdaptiveAvgPool2d(1),
                nn.Conv2d(embed_dim, embed_dim // 4, kernel_size=1),
                nn.GELU(),
                nn.Conv2d(embed_dim // 4, embed_dim, kernel_size=1),
                nn.Sigmoid()
            ),
            # è·¯å¾„3: ä¸Šä¸‹æ–‡ç‰¹å¾å¢žå¼º
            nn.Sequential(
                nn.Conv2d(embed_dim, embed_dim // 2, kernel_size=1),
                nn.BatchNorm2d(embed_dim // 2),
                nn.GELU(),
                nn.Conv2d(embed_dim // 2, embed_dim, kernel_size=1)
            )
        ])
        
        # ç‰¹å¾è‡ªæ ¡å‡†æœºåˆ¶
        self.feature_calibration = nn.Sequential(
            nn.Linear(embed_dim, embed_dim // 2),
            nn.GELU(),
            nn.Linear(embed_dim // 2, embed_dim),
            nn.Sigmoid()
        )
        
        # å¤šçº§ç‰¹å¾ç»†åŒ–
        self.refinement_stages = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1),
                nn.BatchNorm2d(embed_dim),
                nn.GELU()
            ) for _ in range(3)
        ])
        
        # æœ€ç»ˆèžåˆ
        self.final_fusion = nn.Sequential(
            nn.Conv2d(embed_dim * 3, embed_dim, kernel_size=1),
            nn.BatchNorm2d(embed_dim),
            nn.GELU()
        )
        
    def forward(self, features: torch.Tensor) -> torch.Tensor:
        """
        Args:
            features: (B, N, D) æˆ– (B, D, H, W)
        Returns:
            å¢žå¼ºåŽçš„ç‰¹å¾
        """
        # è½¬æ¢ä¸ºç©ºé—´æ ¼å¼
        if len(features.shape) == 3:
            B, N, D = features.shape
            H = W = int(math.sqrt(N))
            spatial_feat = features.transpose(1, 2).contiguous().view(B, D, H, W)
            return_seq = True
        else:
            B, D, H, W = features.shape
            spatial_feat = features.contiguous()
            return_seq = False
        
        # å¤šè·¯å¾„ç‰¹å¾å¢žå¼º
        enhanced_paths = []
        for path in self.enhancement_paths:
            enhanced = path(spatial_feat)
            if enhanced.shape[2:] != (H, W):
                enhanced = F.interpolate(enhanced, size=(H, W), mode='bilinear', align_corners=False)
            enhanced_paths.append(enhanced.contiguous())
        
        # è·¯å¾„2æ˜¯æ³¨æ„åŠ›æƒé‡ï¼Œåº”ç”¨åˆ°å…¶ä»–è·¯å¾„
        attention_weight = enhanced_paths[1]  # (B, D, 1, 1)
        enhanced_paths[0] = enhanced_paths[0] * attention_weight
        enhanced_paths[2] = enhanced_paths[2] * attention_weight
        
        # ç‰¹å¾èžåˆ
        enhanced_combined = torch.cat(enhanced_paths, dim=1).contiguous()  # (B, D*3, H, W)
        enhanced_fused = self.final_fusion(enhanced_combined).contiguous()  # (B, D, H, W)
        
        # ç‰¹å¾è‡ªæ ¡å‡†
        seq_feat = enhanced_fused.view(B, D, H*W).transpose(1, 2).contiguous()  # (B, H*W, D)
        calibration_weights = self.feature_calibration(seq_feat).contiguous()  # (B, H*W, D)
        calibrated_feat = (seq_feat * calibration_weights).contiguous()
        calibrated_spatial = calibrated_feat.transpose(1, 2).contiguous().view(B, D, H, W)
        
        # å¤šçº§ç‰¹å¾ç»†åŒ–
        refined_feat = calibrated_spatial.contiguous()
        for refinement in self.refinement_stages:
            refined_feat = refined_feat + refinement(refined_feat)  # æ®‹å·®è¿žæŽ¥
        
        # è¿”å›žä¸ŽåŽŸæ ¼å¼ä¸€è‡´
        if return_seq:
            return refined_feat.view(B, D, N).transpose(1, 2).contiguous()
        else:
            return refined_feat.contiguous()

class AdaptiveModalityBalancing(nn.Module):
    """è‡ªé€‚åº”æ¨¡æ€å¹³è¡¡æ¨¡å— - é¦–æ¬¡æå‡º
    è§£å†³æ¨¡æ€ä¸å¹³è¡¡é—®é¢˜ï¼šé˜²æ­¢å¼ºæ¨¡æ€ç¢¾åŽ‹å¼±æ¨¡æ€ï¼Œè®©æ¨¡åž‹å……åˆ†åˆ©ç”¨æ‰€æœ‰æ¨¡æ€ä¿¡æ¯
    
    åˆ›æ–°ç‚¹ï¼š
    1. åŠ¨æ€è¯†åˆ«"å­¦éœ¸"æ¨¡æ€ï¼šæ£€æµ‹å“ªä¸ªæ¨¡æ€è´¡çŒ®æœ€å¤§
    2. å¼ºæ¨¡æ€ç­–ç•¥ï¼šè®©å…¶åœ¨æ›´å´Žå²–çš„æŸå¤±å¹³é¢ä¸Šå¯»æ‰¾æ›´å¹³å¦çš„è§£
       - æ¢¯åº¦å¹³æ»‘/æƒ©ç½šï¼Œæå‡é²æ£’æ€§
       - é˜²æ­¢æ­»è®°ç¡¬èƒŒï¼ˆè¿‡æ‹Ÿåˆï¼‰
    3. å¼±æ¨¡æ€ç­–ç•¥ï¼šè®©å…¶åœ¨ç›¸å¯¹å¹³ç¼“çš„åŒºåŸŸè‡ªç”±æŽ¢ç´¢
       - æ¢¯åº¦æ”¾å¤§ï¼Œæ›´å®¹æ˜“å­¦ä¹ 
       - è´¡çŒ®è‡ªå·±ç‹¬ç‰¹çš„ä¿¡æ¯
    """
    
    def __init__(self, embed_dim: int, num_modalities: int = 2, 
                 contribution_threshold: float = 0.6, 
                 strong_modality_smoothness: float = 0.1,
                 weak_modality_boost: float = 1.5):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_modalities = num_modalities
        self.contribution_threshold = contribution_threshold
        self.strong_modality_smoothness = strong_modality_smoothness
        self.weak_modality_boost = weak_modality_boost
        
        # æ¨¡æ€è´¡çŒ®åº¦æ£€æµ‹ç½‘ç»œï¼ˆåŸºäºŽç‰¹å¾æ¿€æ´»å’Œæ¢¯åº¦ï¼‰
        self.contribution_detector = nn.ModuleList([
            nn.Sequential(
                nn.Linear(embed_dim, embed_dim // 2),
                nn.GELU(),
                nn.Linear(embed_dim // 2, 1),
                nn.Sigmoid()
            ) for _ in range(num_modalities)
        ])
        
        # æ¢¯åº¦å¹³æ»‘ç½‘ç»œï¼ˆç”¨äºŽå¼ºæ¨¡æ€ï¼‰
        self.gradient_smoother = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1),
                nn.BatchNorm2d(embed_dim),
                nn.GELU()
            ) for _ in range(num_modalities)
        ])
        
        # ç‰¹å¾é‡è¦æ€§å¢žå¼ºï¼ˆç”¨äºŽå¼±æ¨¡æ€ï¼‰
        self.weak_modality_enhancer = nn.ModuleList([
            nn.Sequential(
                nn.Linear(embed_dim, embed_dim),
                nn.LayerNorm(embed_dim),
                nn.GELU(),
                nn.Linear(embed_dim, embed_dim)
            ) for _ in range(num_modalities)
        ])
        
        # è‡ªé€‚åº”æƒé‡ç”Ÿæˆå™¨
        self.adaptive_weight_generator = nn.Sequential(
            nn.Linear(embed_dim * num_modalities + num_modalities, embed_dim),  # +num_modalities for contribution scores
            nn.GELU(),
            nn.Linear(embed_dim, num_modalities),
            nn.Softmax(dim=-1)
        )
        
        # å­˜å‚¨è´¡çŒ®åº¦ï¼ˆç”¨äºŽè®­ç»ƒæ—¶çš„æ¢¯åº¦è°ƒæ•´ï¼‰
        self.register_buffer('last_contributions', torch.zeros(num_modalities))
        
    def forward(self, modality_features: List[torch.Tensor], 
                return_contributions: bool = False) -> Union[Tuple[List[torch.Tensor], torch.Tensor], List[torch.Tensor]]:
        """
        Args:
            modality_features: å„æ¨¡æ€ç‰¹å¾åˆ—è¡¨ [(B, N, D), ...]
            return_contributions: æ˜¯å¦è¿”å›žè´¡çŒ®åº¦åˆ†æ•°
        Returns:
            å¦‚æžœreturn_contributions=True:
                (balanced_features_list, contributions): 
                    - balanced_features_list: å¹³è¡¡åŽçš„å„æ¨¡æ€ç‰¹å¾åˆ—è¡¨ [(B, N, D), ...]
                    - contributions: å„æ¨¡æ€è´¡çŒ®åº¦ (num_modalities,)
            å¦‚æžœreturn_contributions=False:
                balanced_features_list: å¹³è¡¡åŽçš„å„æ¨¡æ€ç‰¹å¾åˆ—è¡¨ [(B, N, D), ...]
        """
        assert len(modality_features) == self.num_modalities, \
            f"Expected {self.num_modalities} modalities, got {len(modality_features)}"
        
        B, N, D = modality_features[0].shape
        
        # 1. è®¡ç®—å„æ¨¡æ€è´¡çŒ®åº¦ï¼ˆåŸºäºŽç‰¹å¾æ¿€æ´»å€¼ï¼‰
        contributions = []
        for i, mod_feat in enumerate(modality_features):
            # ä½¿ç”¨è´¡çŒ®åº¦æ£€æµ‹å™¨
            contrib_score = self.contribution_detector[i](mod_feat).contiguous()  # (B, N, 1)
            # å¹³å‡æ± åŒ–å¾—åˆ°å…¨å±€è´¡çŒ®åº¦
            avg_contrib = torch.mean(contrib_score).item()
            contributions.append(avg_contrib)
        
        contributions = torch.tensor(contributions, device=modality_features[0].device)
        contributions_normalized = F.softmax(contributions, dim=0)  # å½’ä¸€åŒ–
        
        # æ›´æ–°å­˜å‚¨çš„è´¡çŒ®åº¦
        self.last_contributions = contributions_normalized.detach()
        
        # 2. è¯†åˆ«å¼ºæ¨¡æ€å’Œå¼±æ¨¡æ€
        strong_modality_mask = contributions_normalized > self.contribution_threshold
        weak_modality_mask = ~strong_modality_mask
        
        # 3. å¯¹å¼ºæ¨¡æ€ï¼šåº”ç”¨æ¢¯åº¦å¹³æ»‘ï¼ˆå¯»æ‰¾å¹³å¦è§£ï¼‰
        # å¯¹å¼±æ¨¡æ€ï¼šåº”ç”¨ç‰¹å¾å¢žå¼ºï¼ˆæ›´å®¹æ˜“å­¦ä¹ ï¼‰
        processed_features = []
        for i, mod_feat in enumerate(modality_features):
            if strong_modality_mask[i]:
                # å¼ºæ¨¡æ€ï¼šåº”ç”¨å¹³æ»‘æ“ä½œï¼ˆæ¨¡æ‹Ÿæ¢¯åº¦å¹³æ»‘çš„æ•ˆæžœï¼‰
                # è½¬æ¢åˆ°ç©ºé—´æ ¼å¼è¿›è¡Œå¹³æ»‘
                H = W = int(math.sqrt(N))
                spatial_feat = mod_feat.transpose(1, 2).contiguous().view(B, D, H, W)
                smoothed_feat = self.gradient_smoother[i](spatial_feat).contiguous()
                # æ··åˆåŽŸå§‹å’Œå¹³æ»‘ç‰¹å¾ï¼ˆé˜²æ­¢è¿‡åº¦å¹³æ»‘ï¼‰
                balanced_feat = (1 - self.strong_modality_smoothness) * spatial_feat + \
                               self.strong_modality_smoothness * smoothed_feat
                # è½¬å›žåºåˆ—æ ¼å¼
                processed_feat = balanced_feat.view(B, D, N).transpose(1, 2).contiguous()
            else:
                # å¼±æ¨¡æ€ï¼šåº”ç”¨ç‰¹å¾å¢žå¼ºï¼ˆæ”¾å¤§æ¢¯åº¦æ•ˆåº”ï¼‰
                enhanced_feat = self.weak_modality_enhancer[i](mod_feat).contiguous()
                # æ··åˆåŽŸå§‹å’Œå¢žå¼ºç‰¹å¾ï¼ˆæ ¹æ®è´¡çŒ®åº¦è°ƒæ•´æ··åˆæ¯”ä¾‹ï¼‰
                boost_factor = self.weak_modality_boost * (1.0 - contributions_normalized[i].item())
                processed_feat = mod_feat + boost_factor * (enhanced_feat - mod_feat)
                processed_feat = processed_feat.contiguous()
            
            processed_features.append(processed_feat)
        
        # 4. è‡ªé€‚åº”åŠ æƒèžåˆï¼ˆæ ¹æ®è´¡çŒ®åº¦åŠ¨æ€è°ƒæ•´æƒé‡ï¼‰
        # æž„å»ºè¾“å…¥ï¼šç‰¹å¾ + è´¡çŒ®åº¦åˆ†æ•°
        concat_features = torch.cat(processed_features, dim=-1).contiguous()  # (B, N, D*num_modalities)
        contrib_expanded = contributions_normalized.unsqueeze(0).unsqueeze(0).expand(B, N, -1).contiguous()  # (B, N, num_modalities)
        fusion_input = torch.cat([concat_features, contrib_expanded], dim=-1).contiguous()
        
        # ç”Ÿæˆè‡ªé€‚åº”æƒé‡
        adaptive_weights = self.adaptive_weight_generator(fusion_input).contiguous()  # (B, N, num_modalities)
        
        # è¿”å›žå¹³è¡¡åŽçš„å„æ¨¡æ€ç‰¹å¾ï¼ˆä¸èžåˆï¼Œä¿æŒå„æ¨¡æ€ç‹¬ç«‹ï¼‰
        # è¿™æ ·å¯ä»¥è®©åŽç»­æ¨¡å—ç»§ç»­åˆ©ç”¨å„æ¨¡æ€çš„ç‹¬ç‰¹ä¿¡æ¯
        balanced_features_list = []
        for i, feat in enumerate(processed_features):
            # åº”ç”¨è‡ªé€‚åº”æƒé‡è¿›è¡Œè½»å¾®è°ƒæ•´ï¼ˆä¸æ”¹å˜æ¨¡æ€ç‹¬ç«‹æ€§ï¼‰
            weight = adaptive_weights[:, :, i:i+1].contiguous()
            # åŠ æƒå¢žå¼ºï¼šæƒé‡é«˜çš„æ¨¡æ€å¾—åˆ°æ›´å¤šå…³æ³¨ï¼Œä½†ä¿ç•™åŽŸå§‹ç‰¹å¾
            balanced_feat = (0.8 * feat + 0.2 * weight * feat).contiguous()
            balanced_features_list.append(balanced_feat)
        
        if return_contributions:
            return balanced_features_list, contributions_normalized
        else:
            return balanced_features_list
    
    def get_modality_contributions(self) -> torch.Tensor:
        """èŽ·å–ä¸Šæ¬¡è®¡ç®—çš„æ¨¡æ€è´¡çŒ®åº¦"""
        return self.last_contributions.clone()

class CrossModalAttention(nn.Module):
    """è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶"""
    
    def __init__(self, embed_dim: int, num_heads: int = 8):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.out_proj = nn.Linear(embed_dim, embed_dim)
        
        self.scale = self.head_dim ** -0.5
        # é’©å­ï¼šç¼“å­˜æœ€è¿‘ä¸€æ¬¡çš„æ³¨æ„åŠ›çŸ©é˜µ (B, heads, N, N)
        self.last_attn = None
    
    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor) -> torch.Tensor:
        B, N, D = query.shape
        
        # è®¡ç®—Q, K, V
        q = self.q_proj(query).view(B, N, self.num_heads, self.head_dim).transpose(1, 2).contiguous()
        k = self.k_proj(key).view(B, N, self.num_heads, self.head_dim).transpose(1, 2).contiguous()
        v = self.v_proj(value).view(B, N, self.num_heads, self.head_dim).transpose(1, 2).contiguous()
        
        # è®¡ç®—æ³¨æ„åŠ›
        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = F.softmax(attn, dim=-1)
        # è®°å½•æ³¨æ„åŠ›çŸ©é˜µ
        self.last_attn = attn.detach()
        
        # åº”ç”¨æ³¨æ„åŠ›
        out = (attn @ v).transpose(1, 2).contiguous().view(B, N, D)
        
        return self.out_proj(out).contiguous()

class SpatialSpectralAttention(nn.Module):
    """ç©ºé—´-å…‰è°±æ³¨æ„åŠ›æœºåˆ¶ - é’ˆå¯¹é¥æ„Ÿæ•°æ®ç‰¹ç‚¹è®¾è®¡"""
    
    def __init__(self, embed_dim: int, num_heads: int = 8):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        
        # ç©ºé—´æ³¨æ„åŠ›
        self.spatial_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=False)
        
        # å…‰è°±æ³¨æ„åŠ›
        self.spectral_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=False)
        
        # èžåˆå±‚
        self.fusion = nn.Sequential(
            nn.Linear(embed_dim * 2, embed_dim),
            nn.LayerNorm(embed_dim),
            nn.GELU()
        )
        # é’©å­ï¼šç¼“å­˜æœ€è¿‘ä¸€æ¬¡ç©ºé—´/å…‰è°±æ³¨æ„åŠ›æƒé‡ (B, N, N)ï¼ˆå¹³å‡headæƒé‡ï¼‰
        self.last_spatial_weights = None
        self.last_spectral_weights = None
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (B, N, D)
        B, N, D = x.shape
        x_seq = x.permute(1, 0, 2).contiguous().clone()  # [N, B, D]
        spatial_out, spatial_w = self.spatial_attn(x_seq, x_seq, x_seq)
        spectral_out, spectral_w = self.spectral_attn(x_seq, x_seq, x_seq)
        # è®°å½•æ³¨æ„åŠ›æƒé‡ï¼ˆå½¢çŠ¶ä¸º [B, N, N] æˆ– [N, N]ï¼Œä¸åŒPyTorchç‰ˆæœ¬å¯èƒ½ä¸åŒï¼Œè¿™é‡Œç›´æŽ¥ç¼“å­˜åŽŸæ ·ï¼‰
        self.last_spatial_weights = spatial_w.detach() if isinstance(spatial_w, torch.Tensor) else None
        self.last_spectral_weights = spectral_w.detach() if isinstance(spectral_w, torch.Tensor) else None
        # å›žè½¬è¾“å‡º
        spatial_out = spatial_out.permute(1, 0, 2).contiguous()
        spectral_out = spectral_out.permute(1, 0, 2).contiguous()
        fused = torch.cat([spatial_out, spectral_out], dim=-1).contiguous()
        return self.fusion(fused).contiguous()

class AdaptiveFusionWeights(nn.Module):
    """è‡ªé€‚åº”èžåˆæƒé‡ - æ ¹æ®æ•°æ®è´¨é‡åŠ¨æ€è°ƒæ•´èžåˆç­–ç•¥"""
    
    def __init__(self, embed_dim: int, num_modalities: int = 2):
        super().__init__()
        self.num_modalities = num_modalities
        
        # è´¨é‡è¯„ä¼°ç½‘ç»œ
        self.quality_assessor = nn.Sequential(
            nn.Linear(embed_dim, embed_dim // 2),
            nn.GELU(),
            nn.Linear(embed_dim // 2, 1),
            nn.Sigmoid()
        )
        
        # èžåˆæƒé‡ç”Ÿæˆå™¨
        self.weight_generator = nn.Sequential(
            nn.Linear(embed_dim * num_modalities, embed_dim),
            nn.GELU(),
            nn.Linear(embed_dim, num_modalities),
            nn.Softmax(dim=-1)
        )
        
    def forward(self, features: List[torch.Tensor]) -> torch.Tensor:
        # ç¡®ä¿æ‰€æœ‰ç‰¹å¾tensoréƒ½æ˜¯è¿žç»­çš„
        features = [feat.contiguous() for feat in features]
        
        # è¯„ä¼°å„æ¨¡æ€è´¨é‡
        quality_scores = [self.quality_assessor(feat) for feat in features]
        
        # ç”Ÿæˆèžåˆæƒé‡
        concat_features = torch.cat(features, dim=-1).contiguous()
        fusion_weights = self.weight_generator(concat_features)
        
        # åŠ æƒèžåˆ - ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹å¼
        fused = torch.zeros_like(features[0])
        weights = fusion_weights.unbind(-1)
        for i, (w, feat) in enumerate(zip(weights, features)):
            w_expanded = w.unsqueeze(-1).contiguous()
            feat_contiguous = feat.contiguous()
            fused = fused + w_expanded * feat_contiguous
        
        return fused.contiguous(), fusion_weights, quality_scores

class MultiScaleContextAggregator(nn.Module):
    """å¤šå°ºåº¦ä¸Šä¸‹æ–‡èšåˆå™¨ - é’ˆå¯¹é¥æ„Ÿå¤šå°ºåº¦ç‰¹å¾"""
    
    def __init__(self, embed_dim: int, scales: List[int] = [1, 2, 4, 8]):
        super().__init__()
        self.scales = scales
        self.embed_dim = embed_dim
        
        # å¤šå°ºåº¦ç‰¹å¾æå–å™¨
        self.scale_extractors = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1),
                nn.BatchNorm2d(embed_dim),
                nn.GELU(),
                nn.Conv2d(embed_dim, embed_dim, kernel_size=scale, stride=scale),
                nn.BatchNorm2d(embed_dim),
                nn.GELU()
            ) for scale in scales
        ])
        
        # ä¸Šä¸‹æ–‡èžåˆ
        self.context_fusion = nn.Sequential(
            nn.Conv2d(embed_dim * len(scales), embed_dim, kernel_size=1),
            nn.BatchNorm2d(embed_dim),
            nn.GELU()
        )
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, N, D = x.shape
        H = W = int(math.sqrt(N))
        
        # ç¡®ä¿è¾“å…¥tensoræ˜¯è¿žç»­çš„å¹¶ä¸”æ­£ç¡®å¯¹é½
        x = x.contiguous()
        x_spatial = x.transpose(1, 2).reshape(B, D, H, W).contiguous()
        
        # å¤šå°ºåº¦ç‰¹å¾æå– - æ·»åŠ å†…å­˜å¯¹é½ä¿æŠ¤
        scale_features = []
        for extractor in self.scale_extractors:
            try:
                # ç¡®ä¿è¾“å…¥å¯¹é½
                feat = extractor(x_spatial.contiguous())
                # ä¸Šé‡‡æ ·å›žåŽŸå§‹å¤§å°
                if feat.shape[2] != H or feat.shape[3] != W:
                    feat = F.interpolate(feat.contiguous(), size=(H, W), mode='bilinear', align_corners=False)
                scale_features.append(feat.contiguous())
            except RuntimeError as e:
                # å¦‚æžœå‡ºé”™ï¼Œä½¿ç”¨åŽŸå§‹ç‰¹å¾
                print(f"âš ï¸ MultiScaleContextAggregator error: {e}, using original features")
                scale_features.append(x_spatial.contiguous())
        
        # èžåˆå¤šå°ºåº¦ç‰¹å¾
        concat_features = torch.cat(scale_features, dim=1).contiguous()
        fused = self.context_fusion(concat_features).contiguous()
        
        # è½¬æ¢å›žåºåˆ—æ ¼å¼ - ç¡®ä¿å†…å­˜å¯¹é½
        return fused.reshape(B, D, N).transpose(1, 2).contiguous()

class ResidualBlock(nn.Module):
    """ç®€åŒ–çš„æ®‹å·®å—"""
    def __init__(self, in_channels, out_channels, stride=1, downsample=None):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channels)
        
        # è‡ªåŠ¨åˆ›å»ºdownsampleå¦‚æžœé€šé“æ•°æˆ–strideæ”¹å˜
        if downsample is None:
            if stride != 1 or in_channels != out_channels:
                self.downsample = nn.Sequential(
                    nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),
                    nn.BatchNorm2d(out_channels)
                )
            else:
                self.downsample = None
        else:
            self.downsample = downsample
        
        self.stride = stride
        
    def forward(self, x):
        identity = x
        
        out = self.conv1(x)
        out = self.bn1(out)
        out = F.relu(out)
        
        out = self.conv2(out)
        out = self.bn2(out)
        
        if self.downsample is not None:
            identity = self.downsample(x)
        
        out += identity
        out = F.relu(out)
        
        return out

class ConvNeXtFeatureExtractor(nn.Module):
    """ConvNeXt-Tç‰¹å¾æå–å™¨ - çŽ°ä»£åŒ–CNNæž¶æž„ï¼Œé¢„æœŸ+4~5% mIoU"""
    
    def __init__(self, in_channels: int, out_channels: int = 256, pretrained: bool = False):
        super().__init__()
        
        self.use_convnext = False
        
        # DataParallelæ¨¡å¼ä¸‹å¼ºåˆ¶ä½¿ç”¨ç®€åŒ–ResNet (ConvNeXtä¸å…¼å®¹)
        # å¦‚æžœéœ€è¦ConvNeXtï¼Œè¯·ä½¿ç”¨DDPæ¨¡å¼ (torchrun)
        
        # ä½¿ç”¨ç®€åŒ–ResNet (DataParallelç¨³å®šæ¨¡å¼)
        if in_channels != 3:
            print(f"âœ… ä½¿ç”¨ç®€åŒ–ResNet backbone for {in_channels}-channel input (DataParallelç¨³å®š)")
        else:
            print("âœ… ä½¿ç”¨ç®€åŒ–ResNet backbone for RGB (DataParallelç¨³å®š)")
            
        self.conv1 = nn.Sequential(
            nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        )
        self.layer1 = nn.Sequential(
            ResidualBlock(64, 128, stride=2),
            ResidualBlock(128, 128, stride=1)
        )
        self.layer2 = nn.Sequential(
            ResidualBlock(128, 256, stride=2),
            ResidualBlock(256, 256, stride=1)
        )
        self.layer3 = nn.Sequential(
            ResidualBlock(256, 512, stride=2),
            ResidualBlock(512, 512, stride=1)
        )
        self.output_proj = nn.Conv2d(512, out_channels, kernel_size=1)
        
    def forward(self, x):
        if self.use_convnext:
            # ConvNeXtå‰å‘ä¼ æ’­
            features = self.backbone(x)
            x = features[0]  # (B, 768, H/32, W/32)
            x = self.output_proj(x)  # (B, out_channels, H/32, W/32)
        else:
            # ç®€åŒ–ResNetå‰å‘ä¼ æ’­
            x = self.conv1(x)
            x = self.layer1(x)
            x = self.layer2(x)
            x = self.layer3(x)
            x = self.output_proj(x)
        return x

# ä¿ç•™æ—§åç§°ä»¥å…¼å®¹
ResNetFeatureExtractor = ConvNeXtFeatureExtractor

class BoundaryRefinementModule(nn.Module):
    """è¾¹ç•Œç»†åŒ–æ¨¡å— - å‡å°‘è¯¯åˆ†é”™åˆ†"""
    
    def __init__(self, in_channels: int, num_classes: int):
        super().__init__()
        self.boundary_conv = nn.Sequential(
            nn.Conv2d(in_channels, in_channels // 2, kernel_size=3, padding=1),
            nn.BatchNorm2d(in_channels // 2),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels // 2, in_channels // 4, kernel_size=3, padding=1),
            nn.BatchNorm2d(in_channels // 4),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels // 4, num_classes, kernel_size=1)
        )
        
    def forward(self, features, logits):
        # è®¡ç®—è¾¹ç•Œæ³¨æ„åŠ›
        boundary_attn = self.boundary_conv(features)
        # è¾¹ç•Œå¢žå¼º
        refined_logits = logits + 0.2 * boundary_attn
        return refined_logits

class FPNDecoder(nn.Module):
    """FPNå¼è§£ç å™¨ - å¤šå°ºåº¦ç‰¹å¾èžåˆ"""
    
    def __init__(self, in_channels: int, num_classes: int):
        super().__init__()
        self.in_channels = in_channels
        
        # æ¨ªå‘è¿žæŽ¥
        self.lateral_convs = nn.ModuleList([
            nn.Conv2d(in_channels, 256, kernel_size=1) for _ in range(4)
        ])
        
        # ç‰¹å¾èžåˆ
        self.fpn_convs = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(256, 256, kernel_size=3, padding=1),
                nn.BatchNorm2d(256),
                nn.ReLU(inplace=True)
            ) for _ in range(4)
        ])
        
        # æœ€ç»ˆèžåˆå’Œåˆ†ç±»
        self.final_fusion = nn.Sequential(
            nn.Conv2d(256 * 4, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, num_classes, kernel_size=1)
        )
        
    def forward(self, features_list):
        # æž„å»ºFPN
        fpn_features = []
        prev_feature = None
        
        for i, (lateral_conv, fpn_conv, feature) in enumerate(zip(
            self.lateral_convs, self.fpn_convs, features_list[::-1]
        )):
            lateral = lateral_conv(feature)
            
            if prev_feature is not None:
                # ä¸Šé‡‡æ ·å‰ä¸€ç‰¹å¾
                prev_feature = F.interpolate(
                    prev_feature, size=lateral.shape[2:], 
                    mode='bilinear', align_corners=False
                )
                lateral = lateral + prev_feature
            
            fpn_feature = fpn_conv(lateral)
            fpn_features.append(fpn_feature)
            prev_feature = fpn_feature
        
        # ä¸Šé‡‡æ ·æ‰€æœ‰ç‰¹å¾åˆ°ç›¸åŒå°ºå¯¸
        target_size = fpn_features[-1].shape[2:]
        upsampled_features = []
        for feature in fpn_features:
            if feature.shape[2:] != target_size:
                feature = F.interpolate(
                    feature, size=target_size, 
                    mode='bilinear', align_corners=False
                )
            upsampled_features.append(feature)
        
        # èžåˆæ‰€æœ‰å°ºåº¦ç‰¹å¾
        fused = torch.cat(upsampled_features, dim=1)
        output = self.final_fusion(fused)
        
        return output

class EnhancedMultimodalFramework(nn.Module):
    """å¢žå¼ºç‰ˆå¤šæ¨¡æ€èžåˆæ¡†æž¶"""
    
    def __init__(self, rgb_channels: int = 3, dsm_channels: int = 1, 
                 num_classes: int = 6, embed_dim: int = 128,
                 enable_remote_sensing_innovations: bool = True,
                 pretrained: bool = False,
                 use_multi_scale_aggregator: bool = False,
                 use_simple_mode: bool = False):
        super().__init__()
        
        self.embed_dim = embed_dim
        self.num_classes = num_classes
        self.enable_rs_innovations = enable_remote_sensing_innovations
        self.use_multi_scale_aggregator = use_multi_scale_aggregator
        self.use_simple_mode = use_simple_mode
        
        if use_simple_mode:
            print("ðŸ”§ ç®€åŒ–æ¨¡å¼å·²å¯ç”¨ - æé«˜DataParallelç¨³å®šæ€§")
        
        # ConvNeXtç‰¹å¾æå–å™¨ï¼ˆæ ¸å¿ƒæ¨¡å—ï¼‰
        self.rgb_backbone = ResNetFeatureExtractor(rgb_channels, embed_dim, pretrained=pretrained)
        self.dsm_backbone = ResNetFeatureExtractor(dsm_channels, embed_dim, pretrained=pretrained)
        
        # è·¨æ¨¡æ€æ³¨æ„åŠ›ï¼ˆæ ¸å¿ƒèžåˆæ¨¡å—ï¼‰
        self.cross_attention = CrossModalAttention(embed_dim)
        
        # å·²ç§»é™¤ï¼šæ—§çš„MultiGranularityFusionï¼Œç”±MultiGranularityConsistencyFusionæ›¿ä»£
        
        # é¥æ„Ÿç‰¹å¼‚æ€§åˆ›æ–°æ¨¡å—
        if self.enable_rs_innovations:
            # ç©ºé—´-å…‰è°±æ³¨æ„åŠ›
            self.spatial_spectral_attn = SpatialSpectralAttention(embed_dim)
            
            # è‡ªé€‚åº”èžåˆæƒé‡
            self.adaptive_fusion = AdaptiveFusionWeights(embed_dim, num_modalities=2)
            
            # å¤šå°ºåº¦ä¸Šä¸‹æ–‡èšåˆ (å¯é€‰ï¼ŒDataParallelä¸‹å¯èƒ½æœ‰é—®é¢˜)
            if self.use_multi_scale_aggregator:
                self.multi_scale_aggregator = MultiScaleContextAggregator(embed_dim)
                print("âš ï¸ MultiScaleContextAggregatorå·²å¯ç”¨ (å¯èƒ½å¯¼è‡´DataParallelé—®é¢˜)")
            else:
                self.multi_scale_aggregator = None
                print("âœ… MultiScaleContextAggregatorå·²ç¦ç”¨ (é¿å…DataParallelé—®é¢˜)")
            
        # è‡ªé€‚åº”æ¨¡æ€å¹³è¡¡æ¨¡å—ï¼ˆåˆ›æ–°1ï¼šé˜²æ­¢å¼ºæ¨¡æ€ç¢¾åŽ‹å¼±æ¨¡æ€ï¼‰â­
        self.modality_balancing = AdaptiveModalityBalancing(
            embed_dim=embed_dim,
            num_modalities=2,
            contribution_threshold=0.6,
            strong_modality_smoothness=0.1,
            weak_modality_boost=1.5
        )
        
        # å¤šç²’åº¦ä¸€è‡´æ€§èžåˆï¼ˆåˆ›æ–°2ï¼šåŠ¨æ€ç²’åº¦é€‰æ‹©ï¼‰â­
        self.multi_granularity_consistency = MultiGranularityConsistencyFusion(embed_dim)
        
        # FPNè§£ç å™¨ï¼ˆæ ¸å¿ƒè§£ç æ¨¡å—ï¼‰
        self.fpn_decoder = FPNDecoder(embed_dim, num_classes)
        
        # è¾¹ç•Œç»†åŒ–æ¨¡å—ï¼ˆå‡å°‘è¯¯åˆ†é”™åˆ†ï¼‰
        self.boundary_refinement = BoundaryRefinementModule(embed_dim, num_classes)
        
        # è¾…åŠ©è§£ç å™¨ç”¨äºŽæ·±åº¦ç›‘ç£ï¼ˆæå‡è®­ç»ƒç¨³å®šæ€§ï¼‰
        self.aux_decoder = nn.Sequential(
            nn.Conv2d(embed_dim, embed_dim // 2, kernel_size=3, padding=1),
            nn.BatchNorm2d(embed_dim // 2),
            nn.GELU(),
            nn.Dropout2d(0.1),  # æ·»åŠ Dropout
            nn.Conv2d(embed_dim // 2, num_classes, kernel_size=1)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.trunc_normal_(m.weight, std=0.02)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, rgb: torch.Tensor = None, dsm: torch.Tensor = None, inputs: dict = None, 
                return_intermediate: bool = False) -> Union[torch.Tensor, Tuple[torch.Tensor, dict]]:
        """å‰å‘ä¼ æ’­
        æ”¯æŒä¸¤ç§è°ƒç”¨æ–¹å¼ï¼š
        1) forward(rgb, dsm)
        2) forward(inputs={ 'rgb': Tensor, 'dsm': Tensor, ... })  // é¢„ç•™å¤šæ¨¡æ€æŽ¥å£
        å½“å‰å®žçŽ°å¯¹é¢å¤–æ¨¡æ€åšå ä½æ‰©å±•ï¼ˆå¿½ç•¥æœªçŸ¥æ¨¡æ€ï¼‰ï¼Œä¼˜å…ˆèžåˆRGBä¸ŽDSMã€‚
        """
        if inputs is not None:
            rgb = inputs.get('rgb', rgb)
            dsm = inputs.get('dsm', dsm)
        assert rgb is not None and dsm is not None, "At least 'rgb' and 'dsm' are required in current implementation."
        B, C, H, W = rgb.shape
        
        # ç”¨äºŽå­˜å‚¨ä¸­é—´ç‰¹å¾ï¼ˆç”¨äºŽå¯è§†åŒ–ï¼‰
        intermediate_features = {} if return_intermediate else None
        
        # ç¡®ä¿è¾“å…¥tensoræ˜¯è¿žç»­çš„
        rgb = rgb.contiguous()
        dsm = dsm.contiguous()
        
        # ResNetç‰¹å¾æå–ï¼ˆæ ¸å¿ƒæ¨¡å—ï¼‰
        rgb_features = self.rgb_backbone(rgb)  # (B, embed_dim, H//16, W//16)
        dsm_features = self.dsm_backbone(dsm)  # (B, embed_dim, H//16, W//16)
        
        # å°†ç©ºé—´ç‰¹å¾è½¬ä¸ºtokenåºåˆ—ç”¨äºŽè·¨æ¨¡æ€æ³¨æ„åŠ›
        B, C, H_feat, W_feat = rgb_features.shape
        rgb_tokens = rgb_features.flatten(2).transpose(1, 2).contiguous()  # (B, N, C)
        dsm_tokens = dsm_features.flatten(2).transpose(1, 2).contiguous()  # (B, N, C)
        
        # è·¨æ¨¡æ€æ³¨æ„åŠ›ï¼ˆç›´æŽ¥ä½¿ç”¨ResNetç‰¹å¾ï¼‰
        rgb_attended = self.cross_attention(rgb_tokens, dsm_tokens, dsm_tokens)
        # è®°å½•ç¬¬ä¸€æ¬¡äº¤å‰æ³¨æ„åŠ›æƒé‡ï¼ˆRGB->DSMï¼‰
        if return_intermediate:
            attn_rgb2dsm = getattr(self.cross_attention, 'last_attn', None)
            if isinstance(attn_rgb2dsm, torch.Tensor):
                intermediate_features['cross_attn_rgb_to_dsm'] = attn_rgb2dsm.detach()
        dsm_attended = self.cross_attention(dsm_tokens, rgb_tokens, rgb_tokens)
        # è®°å½•ç¬¬äºŒæ¬¡äº¤å‰æ³¨æ„åŠ›æƒé‡ï¼ˆDSM->RGBï¼‰
        if return_intermediate:
            attn_dsm2rgb = getattr(self.cross_attention, 'last_attn', None)
            if isinstance(attn_dsm2rgb, torch.Tensor):
                intermediate_features['cross_attn_dsm_to_rgb'] = attn_dsm2rgb.detach()
        
        # ç¡®ä¿attended featuresæ˜¯è¿žç»­çš„
        rgb_attended = rgb_attended.contiguous()
        dsm_attended = dsm_attended.contiguous()
        
        # ä¿å­˜ä¸­é—´ç‰¹å¾ç”¨äºŽå¯è§†åŒ–
        if return_intermediate:
            intermediate_features['rgb_attended'] = rgb_attended.detach()
            intermediate_features['dsm_attended'] = dsm_attended.detach()
        
        # é¥æ„Ÿç‰¹å¼‚æ€§å¤„ç†
        if self.enable_rs_innovations and not self.use_simple_mode:
            # ç©ºé—´-å…‰è°±æ³¨æ„åŠ›å¢žå¼º
            rgb_attended = self.spatial_spectral_attn(rgb_attended).contiguous()
            dsm_attended = self.spatial_spectral_attn(dsm_attended).contiguous()
            
            # è‡ªé€‚åº”èžåˆæƒé‡
            fused_features, fusion_weights, quality_scores = self.adaptive_fusion([rgb_attended, dsm_attended])
            
            # ä¿å­˜èžåˆä¿¡æ¯ç”¨äºŽåˆ†æž
            self._last_fusion_weights = fusion_weights
            self._last_quality_scores = quality_scores
        
        # è‡ªé€‚åº”æ¨¡æ€å¹³è¡¡ï¼ˆé˜²æ­¢å¼ºæ¨¡æ€ç¢¾åŽ‹å¼±æ¨¡æ€ï¼Œæå‡é²æ£’æ€§ï¼‰â­
        if not self.use_simple_mode:
            balanced_features_list, modality_contributions = self.modality_balancing(
                [rgb_attended, dsm_attended], 
                return_contributions=True
            )
            rgb_attended = balanced_features_list[0].contiguous()
            dsm_attended = balanced_features_list[1].contiguous()
            self._last_modality_contributions = modality_contributions  # ä¿å­˜ç”¨äºŽåˆ†æž
        else:
            # ç®€åŒ–æ¨¡å¼ï¼šè·³è¿‡æ¨¡æ€å¹³è¡¡
            modality_contributions = None
        
        # ä¿å­˜ä¸­é—´ç‰¹å¾
        if return_intermediate:
            intermediate_features['rgb_balanced'] = rgb_attended.detach()
            intermediate_features['dsm_balanced'] = dsm_attended.detach()
            if modality_contributions is not None:
                intermediate_features['modality_contributions'] = modality_contributions.detach()
        
        # å¤šç²’åº¦ä¸€è‡´æ€§èžåˆï¼ˆåˆ›æ–°æ¨¡å—ï¼šåŠ¨æ€ç²’åº¦é€‰æ‹©ï¼‰â­
        fused_features = self.multi_granularity_consistency(rgb_attended, dsm_attended).contiguous()
        
        # ä¿å­˜ä¸­é—´ç‰¹å¾
        if return_intermediate:
            intermediate_features['after_multi_granularity'] = fused_features.detach()
        
        # è§£ç 
        N = fused_features.shape[1]
        H_out = W_out = int(math.sqrt(N))
        fused_spatial = fused_features.transpose(1, 2).contiguous().view(B, self.embed_dim, H_out, W_out)
        
        # ä¸Šé‡‡æ ·åˆ°åŽŸå§‹å°ºå¯¸
        if H_out != H // 4 or W_out != W // 4:
            fused_spatial = F.interpolate(fused_spatial, size=(H // 4, W // 4), 
                                        mode='bilinear', align_corners=False).contiguous()
        
        # èžåˆResNetç‰¹å¾å’Œtoken-basedç‰¹å¾
        # å°†tokenç‰¹å¾è½¬ä¸ºç©ºé—´ç‰¹å¾
        token_spatial = fused_spatial  # å·²è½¬æ¢ä¸ºç©ºé—´æ ¼å¼
        
        # èžåˆResNetç‰¹å¾å’Œtokenç‰¹å¾
        rgb_spatial = F.interpolate(rgb_features, size=token_spatial.shape[2:], mode='bilinear', align_corners=False)
        dsm_spatial = F.interpolate(dsm_features, size=token_spatial.shape[2:], mode='bilinear', align_corners=False)
        
        # å¤šå°ºåº¦ç‰¹å¾åˆ—è¡¨ç”¨äºŽFPNè§£ç å™¨
        feature_list = [
            F.interpolate(rgb_features, size=(H//16, W//16), mode='bilinear', align_corners=False),
            F.interpolate(rgb_features, size=(H//8, W//8), mode='bilinear', align_corners=False),
            token_spatial,  # (H//4, W//4)
            F.interpolate(token_spatial, size=(H//2, W//2), mode='bilinear', align_corners=False)
        ]
        
        # ä½¿ç”¨FPNè§£ç å™¨ï¼ˆæ›´ç²¾ç¡®çš„å¤šå°ºåº¦èžåˆï¼‰
        output = self.fpn_decoder(feature_list)
        
        # ä¸Šé‡‡æ ·åˆ°åŽŸå§‹å°ºå¯¸
        if output.shape[2:] != (H, W):
            output = F.interpolate(output, size=(H, W), mode='bilinear', align_corners=False).contiguous()
        
        # è¾¹ç•Œç»†åŒ–ï¼ˆå‡å°‘è¯¯åˆ†é”™åˆ†ï¼‰
        refined_output = self.boundary_refinement(
            F.interpolate(token_spatial, size=(H, W), mode='bilinear', align_corners=False),
            output
        )
        
        # è¾…åŠ©è¾“å‡ºç”¨äºŽæ·±åº¦ç›‘ç£ï¼ˆè®­ç»ƒæ—¶ä½¿ç”¨ï¼‰
        if self.training:
            aux_output = self.aux_decoder(token_spatial)
            aux_output = F.interpolate(aux_output, size=(H, W), mode='bilinear', align_corners=False)
            if return_intermediate:
                return (refined_output.contiguous(), aux_output.contiguous()), intermediate_features
            return refined_output.contiguous(), aux_output.contiguous()
        else:
            if return_intermediate:
                return refined_output.contiguous(), intermediate_features
            return refined_output.contiguous()
    
    def get_fusion_weights(self) -> Optional[torch.Tensor]:
        """èŽ·å–å½“å‰èžåˆæƒé‡ï¼ˆç”¨äºŽåˆ†æžï¼‰"""
        if not self.enable_rs_innovations:
            return None
        return getattr(self, '_last_fusion_weights', None)
    
    def get_quality_scores(self) -> Optional[List[torch.Tensor]]:
        """èŽ·å–æ•°æ®è´¨é‡è¯„åˆ†ï¼ˆç”¨äºŽåˆ†æžï¼‰"""
        if not self.enable_rs_innovations:
            return None
        return getattr(self, '_last_quality_scores', None)
    
    def get_modality_contributions(self) -> Optional[torch.Tensor]:
        """èŽ·å–æ¨¡æ€è´¡çŒ®åº¦ï¼ˆç”¨äºŽåˆ†æžæ¨¡æ€å¹³è¡¡æ•ˆæžœï¼‰
        Returns:
            å„æ¨¡æ€è´¡çŒ®åº¦ tensor (num_modalities,)ï¼Œå¦‚ [0.65, 0.35] è¡¨ç¤ºRGBè´¡çŒ®65%ï¼ŒDSMè´¡çŒ®35%
        """
        return getattr(self, '_last_modality_contributions', None)

def count_parameters(model):
    """è®¡ç®—æ¨¡åž‹å‚æ•°é‡"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

if __name__ == "__main__":
    # æµ‹è¯•æ¨¡åž‹
    model = EnhancedMultimodalFramework(
        rgb_channels=3,
        dsm_channels=1,
        num_classes=6,
        embed_dim=256,
        enable_remote_sensing_innovations=True
    )
    
    print(f"æ¨¡åž‹å‚æ•°é‡: {count_parameters(model):,}")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    rgb = torch.randn(2, 3, 256, 256)
    dsm = torch.randn(2, 1, 256, 256)
    
    with torch.no_grad():
        output = model(rgb, dsm)
        print(f"è¾“å…¥å°ºå¯¸: RGB {rgb.shape}, DSM {dsm.shape}")
        print(f"è¾“å‡ºå°ºå¯¸: {output.shape}")
